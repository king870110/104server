import os
from dotenv import load_dotenv
import requests
from datetime import datetime, timedelta
from supabase import create_client

load_dotenv()
SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_KEY")
TABLE = "jobs"
# print("SUPABASE_URL =", SUPABASE_URL)
# print("SUPABASE_KEY =", SUPABASE_KEY)


headers = {
    "apikey": SUPABASE_KEY,
    "Authorization": f"Bearer {SUPABASE_KEY}",
    "Content-Type": "application/json",
    "Prefer": "resolution=merge-duplicates",  # å‘Šè¨´ supabase åš upsert
}


# def upsert_jobs(jobs):
#     url = f"{SUPABASE_URL}/rest/v1/{TABLE}"

#     for job in jobs:
#         job_data = job.copy()

#         if "job_id" not in job_data:
#             print("Error: job data missing 'job_id' key, skip")
#             continue

#         # å¦‚æœè³‡æ–™å·²æœ‰ post_atï¼Œä¸è¦é€å…¥ post_at è®“å®ƒä¸è¢«è¦†å¯«
#         if "posted_at" not in job_data or not job_data["posted_at"]:
#             job_data["posted_at"] = datetime.now().isoformat()

#         params = {"on_conflict": "job_id"}

#         try:
#             res = requests.post(url, headers=headers, json=job_data, params=params)
#             if res.status_code in (200, 201):
#                 print(f"Upsert job {job_data['job_id']} æˆåŠŸ")
#             else:
#                 print(
#                     f"Upsert job {job_data['job_id']} å¤±æ•—: {res.status_code} {res.text}"
#                 )
#         except Exception as e:
#             print(f"Exception when upserting job {job_data['job_id']}: {e}")


supabase = create_client(SUPABASE_URL, SUPABASE_KEY)


def upsert_jobs(jobs):
    for job in jobs:
        job_data = job.copy()

        if "job_id" not in job_data:
            print("âŒ ç¼ºå°‘ job_idï¼Œç•¥é")
            continue

        if not job_data.get("posted_at"):
            job_data["posted_at"] = datetime.now().isoformat()

        try:
            res = supabase.table(TABLE).upsert(job_data, on_conflict="job_id").execute()

            # github
            # if res.status_code in (200, 201):
            #     print(f"âœ… Upsert job {job_data['job_id']} æˆåŠŸ")
            # else:
            #     print(
            #         f"âš ï¸ Upsert job {job_data['job_id']} å¤±æ•—: {res.status_code} {res.data}"
            #     )
            try:
                res = (
                    supabase.table(TABLE)
                    .upsert(job_data, on_conflict="job_id")
                    .execute()
                )
                if res.data:
                    print(f"âœ… Upsert job {job_data['job_id']} æˆåŠŸ")
                else:
                    print(
                        f"âš ï¸ Upsert job {job_data['job_id']} å¯èƒ½å¤±æ•—ï¼Œç„¡å›å‚³è³‡æ–™: {res}"
                    )
            except Exception as e:
                print(f"ğŸ”¥ Exception upserting job {job_data['job_id']}: {e}")

            # local
            # if getattr(res, "error", None):  # v2.x æª¢æŸ¥ error
            #     print(f"âš ï¸ Upsert job {job_data['job_id']} å¤±æ•—: {res.error}")
            # else:
            #     print(f"âœ… Upsert job {job_data['job_id']} æˆåŠŸ: {res.data}")

        except Exception as e:
            print(f"ğŸ”¥ Exception upserting job {job_data['job_id']}: {e}")


def export_json():

    import json

    today = datetime.now()
    start_time = (today - timedelta(days=30)).strftime("%Y-%m-%d %H:%M:%S")
    print("é–‹å§‹æ™‚é–“:", start_time)

    batch_size = 1000
    start = 0
    all_data = []

    while True:
        response = (
            supabase.table("jobs")
            .select("*")
            .gte("created_at", start_time)
            .range(start, start + batch_size - 1)
            .execute()
        )

        if hasattr(response, "error") and response.error:
            print("æŸ¥è©¢éŒ¯èª¤:", response.error)
            break

        batch = response.data
        if not batch:
            break

        all_data.extend(batch)
        print(f"å·²æŠ“å– {len(all_data)} ç­†è³‡æ–™")

        if len(batch) < batch_size:
            # æœ€å¾Œä¸€æ‰¹ï¼ŒçµæŸè¿´åœˆ
            break

        start += batch_size

    if all_data:
        os.makedirs("public", exist_ok=True)
        with open("public/jobs.json", "w", encoding="utf-8") as f:
            json.dump(all_data, f, ensure_ascii=False, indent=2)
        print(f"âœ… æˆåŠŸåŒ¯å‡º {len(all_data)} ç­†è³‡æ–™åˆ° public/jobs.json")
    else:
        print("æ²’æœ‰è³‡æ–™å¯åŒ¯å‡º")
